{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset type : <class 'datasets.arrow_dataset.Dataset'>\n",
      "train dataset : Dataset({\n",
      "    features: ['sentence', 'label', 'idx'],\n",
      "    num_rows: 67349\n",
      "})\n",
      "features in dataset : {'sentence': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive'], id=None), 'idx': Value(dtype='int32', id=None)}\n"
     ]
    }
   ],
   "source": [
    "raw_dataset = load_dataset(\"glue\",\"sst2\")\n",
    "print(\"dataset type :\",type(raw_dataset['train']))\n",
    "print(\"train dataset :\",raw_dataset['train'])\n",
    "print(\"features in dataset :\",raw_dataset['train'].features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': Value(dtype='string', id=None), 'label': ClassLabel(names=['negative', 'positive'], id=None), 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train'].features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'datasets.table.MemoryMappedTable'>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(raw_dataset['train'].data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryMappedTable\n",
       "sentence: string\n",
       "label: int64\n",
       "idx: int32\n",
       "----\n",
       "sentence: [[\"hide new secretions from the parental units \",\"contains no wit , only labored gags \",\"that loves its characters and communicates something rather beautiful about human nature \",\"remains utterly satisfied to remain the same throughout \",\"on the worst revenge-of-the-nerds clichés the filmmakers could dredge up \",...,\"you wish you were at home watching that movie instead of in the theater watching this one \",\"'s no point in extracting the bare bones of byatt 's plot for purposes of bland hollywood romance \",\"underdeveloped \",\"the jokes are flat \",\"a heartening tale of small victories \"],[\"suspense , intriguing characters and bizarre bank robberies , \",\"a gritty police thriller with all the dysfunctional family dynamics one could wish for \",\"with a wonderful ensemble cast of characters that bring the routine day to day struggles of the working class to life \",\"nonetheless appreciates the art and reveals a music scene that transcends culture and race . \",\"do we really need the tiger beat version ? \",...,\"when there 's nothing else happening \",\"on cable \",\"it with ring , \",\"far from a groundbreaking endeavor \",\"that these women are spectacular \"],...,[\"it does turn out to be a bit of a cheat in the end \",\"may be convinced that he has something significant to say \",\"to be both hugely entertaining and uplifting . \",\", boredom never takes hold . \",\"left to work with , sort of like michael jackson 's nose \",...,\"from a severe case of hollywood-itis \",\"the very best of them \",\"thrills , \",\"'s attracting audiences to unfaithful \",\"impressively delicate range \"],[\"starts off promisingly but then proceeds to flop \",\"distinguished actor \",\"on their parents ' anguish \",\"pays off and is effective if you stick with it \",\"is n't particularly funny \",...,\"a delightful comedy \",\"anguish , anger and frustration \",\"at achieving the modest , crowd-pleasing goals it sets for itself \",\"a patient viewer \",\"this new jangle of noise , mayhem and stupidity must be a serious contender for the title . \"]]\n",
       "label: [[0,0,1,0,0,...,0,0,0,0,1],[1,1,1,1,0,...,0,0,1,0,1],...,[0,0,1,1,0,...,0,1,1,1,1],[0,1,0,1,0,...,1,0,1,1,0]]\n",
       "idx: [[0,1,2,3,4,...,995,996,997,998,999],[1000,1001,1002,1003,1004,...,1995,1996,1997,1998,1999],...,[66000,66001,66002,66003,66004,...,66995,66996,66997,66998,66999],[67000,67001,67002,67003,67004,...,67344,67345,67346,67347,67348]]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset['train'].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint  = \"distilbert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laoding pretrained model for tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to see ids allocated to sentences\n",
    "tokenized_sentence = tokenizer(raw_dataset['train'][0:3]['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 5342, 2047, 3595, 8496, 2013, 1996, 18643, 3197, 102], [101, 3397, 2053, 15966, 1010, 2069, 4450, 2098, 18201, 2015, 102], [101, 2008, 7459, 2049, 3494, 1998, 10639, 2015, 2242, 2738, 3376, 2055, 2529, 3267, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(batch):\n",
    "    return tokenizer(batch['sentence'],truncation=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = raw_dataset.map(tokenize_func,batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1:  # if multiple GPUs\n",
    "       print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"sentiment_analysis_trainer\",\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Loading the pretrained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint,num_labels =2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "DistilBertForSequenceClassification                     --\n",
       "├─DistilBertModel: 1-1                                  --\n",
       "│    └─Embeddings: 2-1                                  --\n",
       "│    │    └─Embedding: 3-1                              23,440,896\n",
       "│    │    └─Embedding: 3-2                              393,216\n",
       "│    │    └─LayerNorm: 3-3                              1,536\n",
       "│    │    └─Dropout: 3-4                                --\n",
       "│    └─Transformer: 2-2                                 --\n",
       "│    │    └─ModuleList: 3-5                             42,527,232\n",
       "├─Linear: 1-2                                           590,592\n",
       "├─Linear: 1-3                                           1,538\n",
       "├─Dropout: 1-4                                          --\n",
       "================================================================================\n",
       "Total params: 66,955,010\n",
       "Trainable params: 66,955,010\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to see model parameters before the training\n",
    "params_before = []\n",
    "for name , p in model.named_parameters():\n",
    "    params_before.append(p.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "from datasets import load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"glue\",\"sst2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric.compute(predictions=[1,0,1],references=[1,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(logits_and_labels):\n",
    "    logits,labels = logits_and_labels\n",
    "    predictions = np.argmax(logits,axis=-1)\n",
    "    return metric.compute(predictions=predictions,references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args= training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset= tokenized_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8419' max='8419' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8419/8419 03:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.208500</td>\n",
       "      <td>0.337862</td>\n",
       "      <td>0.909404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8419, training_loss=0.22664838075666136, metrics={'train_runtime': 238.722, 'train_samples_per_second': 282.123, 'train_steps_per_second': 35.267, 'total_flos': 518596929468840.0, 'train_loss': 0.22664838075666136, 'epoch': 1.0})"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"sentiment_analysis_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "our_model = pipeline(\"text-classification\",model='/home/ubuntu/uzair/NLP/sentiment_analysis_model',device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = [\n",
    "    \"I love this product! It's amazing.\",\n",
    "    \"This movie was terrible, I hated it.\",\n",
    "    \"The weather today is pleasant and sunny.\",\n",
    "    \"The food at the restaurant was delicious.\",\n",
    "    \"I'm feeling very happy and excited right now.\",\n",
    "    \"This book is a masterpiece of literature.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'LABEL_1', 'score': 0.9988431930541992}, {'label': 'LABEL_0', 'score': 0.9981188178062439}, {'label': 'LABEL_1', 'score': 0.998943030834198}, {'label': 'LABEL_1', 'score': 0.9987308382987976}, {'label': 'LABEL_1', 'score': 0.9983748197555542}, {'label': 'LABEL_1', 'score': 0.998918890953064}]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model(sample_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference\n",
    "\n",
    "import json\n",
    "\n",
    "config_path = \"/home/ubuntu/uzair/NLP/sentiment_analysis_model/config.json\"\n",
    "with open(config_path) as f:\n",
    "    d = json.load(f)\n",
    "    # print(d)\n",
    "    d['id2label'] = {0:'negative',1: 'positive'}\n",
    "\n",
    "with open(config_path,'w') as f:\n",
    "    json.dump(d,f,indent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.34.0\",\n",
      "  \"vocab_size\": 30522,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"negative\",\n",
      "    \"1\": \"positive\"\n",
      "  }\n",
      "}"
     ]
    }
   ],
   "source": [
    "!cat /home/ubuntu/uzair/NLP/sentiment_analysis_model/config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'positive', 'score': 0.9988431930541992}, {'label': 'negative', 'score': 0.9981188178062439}, {'label': 'positive', 'score': 0.998943030834198}, {'label': 'positive', 'score': 0.9987308382987976}, {'label': 'positive', 'score': 0.9983748197555542}, {'label': 'positive', 'score': 0.998918890953064}]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model(sample_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check if params are changes during training or not\n",
    "params_after = []\n",
    "for name,p in model.named_parameters():\n",
    "    params_after.append(p.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17931.072\n",
      "110.000275\n",
      "2.05167\n",
      "1.2841101\n",
      "1612.3315\n",
      "2.1135306\n",
      "1614.0089\n",
      "0.0036122422\n",
      "1468.5613\n",
      "1.2392111\n",
      "1409.5287\n",
      "0.9639718\n",
      "2.079465\n",
      "0.9605099\n",
      "6195.7256\n",
      "6.9311614\n",
      "5717.6377\n",
      "0.890881\n",
      "1.992882\n",
      "0.86630344\n",
      "1596.4199\n",
      "1.9055786\n",
      "1600.3307\n",
      "0.0035844066\n",
      "1406.9348\n",
      "1.0768696\n",
      "1369.3785\n",
      "0.87917376\n",
      "1.9602393\n",
      "0.8766457\n",
      "6094.748\n",
      "6.4256124\n",
      "5603.518\n",
      "0.8368228\n",
      "1.8786879\n",
      "0.8925983\n",
      "1573.2028\n",
      "1.9689867\n",
      "1583.6948\n",
      "0.0032956232\n",
      "1396.3035\n",
      "0.9630036\n",
      "1374.6539\n",
      "0.8892582\n",
      "1.9563774\n",
      "0.95508695\n",
      "6155.4814\n",
      "7.050356\n",
      "5513.952\n",
      "0.8686353\n",
      "1.7714043\n",
      "0.8424982\n",
      "1561.5447\n",
      "1.7499917\n",
      "1579.2345\n",
      "0.0038739864\n",
      "1428.6957\n",
      "0.9432603\n",
      "1380.324\n",
      "0.9337509\n",
      "1.8230643\n",
      "0.91793245\n",
      "5996.125\n",
      "6.869168\n",
      "5236.5376\n",
      "0.8994397\n",
      "1.691229\n",
      "0.8050519\n",
      "1482.8392\n",
      "1.7797852\n",
      "1482.8049\n",
      "0.0022738744\n",
      "1308.5636\n",
      "0.86920226\n",
      "1296.4889\n",
      "1.0590011\n",
      "1.7743952\n",
      "1.1389029\n",
      "5650.809\n",
      "6.750546\n",
      "4707.041\n",
      "1.0823448\n",
      "1.5869303\n",
      "1.1002113\n",
      "1386.1273\n",
      "1.7020447\n",
      "1414.8899\n",
      "0.0016796269\n",
      "1215.963\n",
      "1.4130276\n",
      "1201.0848\n",
      "1.3824914\n",
      "1.6803751\n",
      "1.534333\n",
      "4955.7896\n",
      "6.573166\n",
      "4486.851\n",
      "1.3337083\n",
      "2.193441\n",
      "0.82992506\n",
      "1081.9757\n",
      "1.2702456\n",
      "2.9737492\n",
      "0.0008775176\n"
     ]
    }
   ],
   "source": [
    "for p1,p2 in zip(params_before,params_after):\n",
    "    print(np.sum(np.abs(p1-p2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
